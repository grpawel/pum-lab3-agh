{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rttMWigaqHiF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.datasets as sk_ds\n",
    "import sklearn.svm as sk_svm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sIyjhUAs3645",
    "outputId": "80d76954-2013-4e2f-b2a4-500b2089d9c1"
   },
   "outputs": [],
   "source": [
    "wbc_features, wbc_targets = sk_ds.load_breast_cancer(True)\n",
    "print('WBC: Zbiór {} danych z {} cechami'.format(len(wbc_targets),\n",
    "                                                 wbc_features.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k39TzniRskcm"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-zWtKtHuYGc"
   },
   "source": [
    "### Klasyfikator liniowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ON5q8fk1xRxR"
   },
   "source": [
    "$$ y = f(\\vec{w} \\cdot \\vec{x}) = f(\\sum_i w_i x_i)  $$\n",
    "\n",
    "$\\vec{w}$ - wektor wag klasyfikatora\n",
    "\n",
    "$\\vec{x}$ - wektor wejściowy cech\n",
    "\n",
    "$f$ - funkcja decyzyjna $\\Re \\to N $ odwzorowuje zbiór liczb rzeczywistych na klasę \n",
    "\n",
    "Dla dwóch klas funkcja $f$ zazwyczaj przypisuje iloczyn skalarny do jednej klasy jeśli jest on większy od pewnej wartości i do drugiej klasy jeśli nie jest. W takim przypadku klasyfikator liniowy dzieli przestrzeń wejściową za pomocą hiperpłaszczyzny, czyli uogólnienia płaszczyzny do n-wymiarów.\n",
    "\n",
    "Dla takiego przypadku hiperpłaszczyzna w klasyfikatorze liniowym może być opisana jako zbiór puntów $\\vec x$ dla których zachodzi $ \\vec w \\cdot \\vec x - b = 0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "BMRzEM2y1ZAB",
    "outputId": "2e5b35cd-24d3-4af6-b567-4e24679241ae"
   },
   "outputs": [],
   "source": [
    "def plot_svm_2d(ax, data, labels, classifier, plot_hyperplanes=False):\n",
    "  min_x = data[:,0].min() - 1\n",
    "  max_x = data[:,0].max() + 1\n",
    "  min_y = data[:,1].min() - 1\n",
    "  max_y = data[:,1].max() + 1\n",
    "  \n",
    "  xx, yy = np.meshgrid(np.arange(min_x, max_x, 0.01),\n",
    "                       np.arange(min_y, max_y, 0.01))\n",
    "  Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z = Z.reshape(xx.shape)\n",
    "  ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "  ax.scatter(data[:,0], data[:,1], c=labels, cmap=plt.cm.coolwarm)\n",
    "  \n",
    "  # Hyperplane\n",
    "  if plot_hyperplanes:\n",
    "    w = classifier.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx2 = np.linspace(min_x, max_x)\n",
    "    yy2 = a * xx2 - (classifier.intercept_[0]) / w[1]\n",
    "  \n",
    "    margin = 1 / np.sqrt(np.sum(classifier.coef_ ** 2))\n",
    "    yy_down = yy2 - np.sqrt(1 + a ** 2) * margin\n",
    "    yy_up = yy2 + np.sqrt(1 + a ** 2) * margin\n",
    "\n",
    "\n",
    "    ax.plot(xx2, yy2, 'k-')\n",
    "    ax.plot(xx2, yy_down, 'k--')\n",
    "    ax.plot(xx2, yy_up, 'k--')\n",
    "  \n",
    "  ax.set_xlim(min_x, max_x)\n",
    "  ax.set_ylim(min_y, max_y)\n",
    "\n",
    "def make_linear(fig, plot_hyperplanes=False,\n",
    "                C=1.0, kernel='linear', **kwargs):\n",
    "  in_features = wbc_features[40:80,:2]\n",
    "  out_targets = wbc_targets[40:80]\n",
    "  \n",
    "  classifier = sk_svm.SVC(kernel=kernel, C=C, **kwargs)\n",
    "  classifier.fit(in_features, out_targets)\n",
    "  plot_svm_2d(fig, in_features, out_targets, classifier, plot_hyperplanes)\n",
    "  return classifier.score(in_features, out_targets)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "score = make_linear(ax)\n",
    "print(score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q86C89Pwub_S"
   },
   "source": [
    "### Klasyfikator SVM\n",
    "\n",
    "SVM (ang. Support Vector Machine) jest to rodzaj klasyfikatora liniowego. Polega on na wyznaczeniu hiperpłaszczyzny tak, aby margines pomiędzy hiperpłaszczyzną i najbliższymi punktami (wektorami nośnymi - support vectors) był jak największy.\n",
    "\n",
    "W tym celu definiujemy dwie dodatkowe hiperpłaszczyzny, równoległe do wspomnianej poprzednio, o równaniach:\n",
    "\n",
    "$$\n",
    "\\vec w \\cdot \\vec x - b = 1\n",
    "$$\n",
    "$$\n",
    "\\vec w \\cdot \\vec x - b = -1\n",
    "$$\n",
    "\n",
    "Odpowiadają one hiperpłaszczyznom granicznym, opartym na wektorach nośnych.\n",
    "\n",
    "Odległość między tym płaszczyznami to $\\frac{2}{\\|\\vec w \\|}$, naszym zadaniem jest więc minimaliacja\n",
    "\n",
    "Minimalizuje $\\|\\vec{w}\\|$ tak aby zachodziło równanie:\n",
    "\n",
    "$$y_i (\\vec{w} \\cdot \\vec{x}_i - b) \\ge 1$$\n",
    "\n",
    ", gdzie \n",
    "\n",
    "$\\vec{w}$ - wektor wag klasyfikatora, \n",
    "\n",
    "$b$ - hiperpłaszczyzna, \n",
    "\n",
    "$\\vec{x}_i$ - punkty zbioru treningowego, \n",
    "\n",
    "$y_i$ - klasa $\\{-1, 1\\}$ do której należy punkt $x_i$\n",
    "\n",
    "Równanie to działa tylko dla danych liniowo separowalnych, dla zbioru nieseparowalnego konieczne jest uogólnienie sformułowania:\n",
    "\n",
    "Zminimalizuj \n",
    "$$\\| w \\|^2 + C \\sum_i \\xi_i $$\n",
    "tak aby zachodziło \n",
    "$$y_i(\\vec w \\cdot \\vec x_i - b) \\ge 1 - \\xi_i,\\ \\xi_i \\ge 0,\\ i=1,...,n$$\n",
    "\n",
    ", gdzie\n",
    "\n",
    "$\\xi_i$ - współczynnik błędu dla danego punktu (*\"jak daleko wchodzi do złej klasy\"*)\n",
    "\n",
    "$C$ - współczynnik określający stosunek między zwiększeniem marginesu, a minimalizacją błędów\n",
    "\n",
    "### Postać dualna\n",
    "\n",
    "\n",
    "Takie sformułowanie oznacza problem znalezienia ekstremum warunkowego. W celu efektywnego rozwiązania tego problemu przy pomocy metod numerycznych stosuje się metodę mnożników Lagrange'a.\n",
    "W tym celu problem przekształcony jest do postaci dualnej:\n",
    "\n",
    "Zakładając, że \n",
    "$$\\vec w = \\sum_i \\alpha_i y_i \\vec x_i$$\n",
    "zmaksymalizuj \n",
    "$$\n",
    "W(\\vec \\alpha)=\\sum_i \\alpha_i - \\frac 1 2 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j (\\vec x_i \\cdot \\vec x_j)\n",
    "$$\n",
    ", tak aby zachodziło $\\sum_i \\alpha_i y_i = 0$ oraz $0\\le \\alpha_i \\le C $ dla $i = 1,..,n$.\n",
    "\n",
    "Jedynymi wektorami dla których $\\alpha_i \\ne 0$ są wektory nośne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "TjdU1C70el0o",
    "outputId": "b0d45149-013b-41ef-c96e-17a206fe799a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4*3, 4.8))\n",
    "Cs = [0.005, 0.05, 0.5, 1.0]\n",
    "for i, c in enumerate(Cs):\n",
    "  ax = fig.add_subplot(1, len(Cs), i + 1)\n",
    "  accuracy = make_linear(ax, True, C=c)\n",
    "  ax.set_title(\"C = {:2.2f}, acc = {:2.2f}\".format(c, accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HR8FkdQYuhhL"
   },
   "source": [
    "### Trik kernelowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EI3yG1I4T3aF"
   },
   "source": [
    "Klasyfkator liniowy, a więc i SVM może poprawnie klasyfikować tylko dane liniowo separowalne, co stanowi bardzo wąską klasę problemów. W tym celu stosuje się **trik kernelowy** oparty na idei, że dane nieseparowalne w *n*-wymiarach mogą być liniowo separowalne w wyższych wymiarach, *n*+1, *n*+2, itd.\n",
    "\n",
    "W tym celu definiujemy nieliniowe przekształcenie punktów wejściowych $\\phi(\\vec x_i)$ oraz funkcję kernela $k(\\vec x_i, \\vec x_j)$ spełniającą zależność $k(\\vec x_i,\\vec x_j ) = \\phi(\\vec x_i) \\cdot \\phi(\\vec x_j)$.\n",
    "\n",
    "Na przykład transformacja dodająca dodatkową współrzędną zawierającą odległość od środka układu współrzędnych:\n",
    "$$\n",
    "\\phi(x): \\Re_2 \\to \\Re_3 \n",
    "$$\n",
    "$$\n",
    "\\phi((x_1, x_2)) = (x_1, x_2, \\sqrt {x_1^2 + x_2^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "tABCzH_GSfpI",
    "outputId": "068f4455-ead4-4509-d290-d3e4c04d21c6"
   },
   "outputs": [],
   "source": [
    "def phi(X):\n",
    "  result = np.zeros((X.shape[0], X.shape[1] + 1))\n",
    "  result[:, :2] = X\n",
    "  result[:, 2] = np.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "  return result\n",
    "\n",
    "def circle_kernel(X, X_fit):\n",
    "  return np.dot(phi(X), phi(X_fit).T)\n",
    "  \n",
    "\n",
    "def circle():\n",
    "  from mpl_toolkits.mplot3d import Axes3D\n",
    "  \n",
    "  coordinates = np.random.randn(40, 2)\n",
    "  labels = np.zeros((40,))\n",
    "  labels[coordinates[:,0]**2 + coordinates[:,1]**2 <= 1.2] = 1\n",
    "  \n",
    "  classifier = classifier = sk_svm.SVC(kernel=circle_kernel)\n",
    "  classifier.fit(coordinates, labels)\n",
    "  \n",
    "  figure = plt.figure(figsize=(6.4*2*1.5, 4.8*1.5))\n",
    "  ax = figure.add_subplot(1, 2, 1)\n",
    "  \n",
    "  plot_svm_2d(ax, coordinates, labels, classifier)\n",
    "  ax.set_title('$\\phi((x_1, x_2)) = (x_1, x_2, \\sqrt{x_1^2 + x_2^2})$')\n",
    "  ax = figure.add_subplot(1, 2, 2, projection='3d')\n",
    "  projected = phi(coordinates)\n",
    "  ax.scatter(projected[:, 0], projected[:,1], projected[:,2], c=labels,\n",
    "             cmap=plt.cm.coolwarm, depthshade=False)\n",
    "  ax.view_init(15, 30)\n",
    "  \n",
    "circle()\n",
    "plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OF9H07x-Z9O1"
   },
   "source": [
    "Podstawiając przekształcenie danych wejściowych do równań dla postaci dualnej, otrzymujemy:\n",
    "\n",
    "Zakładając, że \n",
    "$$\\vec w = \\sum_i \\alpha_i y_i \\phi(\\vec x_i)$$\n",
    "zmaksymalizuj \n",
    "$$\n",
    "W(\\vec \\alpha)=\\sum_i \\alpha_i - \\frac 1 2 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j k(\\vec x_i, \\vec x_j)\n",
    "$$\n",
    ", tak aby zachodziło $\\sum_i \\alpha_i y_i = 0$ oraz $0\\le \\alpha_i \\le C $ dla $i = 1,..,n$.\n",
    "\n",
    "Najczęściej stosowanymi kernelami są:\n",
    "- wielomianowy $k(\\vec x_i, \\vec x_j) = (\\vec x_i \\cdot \\vec x_j + c)^d$\n",
    "- Gaussowski $k(\\vec x_i, \\vec x_j) = \\exp(-\\frac{\\|\\vec x_i - \\vec x_j \\|^2}{2 \\sigma^2})$\n",
    "- sigmoidalny $k(\\vec x_i, \\vec x_j) = \\tanh(\\kappa(\\vec x_i \\cdot \\vec x_j) + \\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KkC1HTcUcRP_",
    "outputId": "c18b3a42-7d50-45c0-d378-5e244dbacd74"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4*3, 4.8))\n",
    "kernels = ['linear', 'poly', 'sigmoid', 'rbf']\n",
    "kwargs = [dict(), dict(), dict(), dict()]\n",
    "kwargs[2]['gamma'] = 0.0008\n",
    "for i, kernel in enumerate(kernels):\n",
    "  ax = fig.add_subplot(1, len(Cs), i + 1)\n",
    "  accuracy = make_linear(ax, kernel=kernel, **kwargs[i])\n",
    "  ax.set_title(\"kernel = '{}', acc = {:2.4f}\".format(kernel, accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OklztgToulNM"
   },
   "source": [
    "### Jakość klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC - Receiver Operating Characteristic \n",
    "Krzywa ROC służy do oceny jakości klasyfikatora. Przedstawia zależność ilości prawdziwie pozytywnych ($T_p$) i fałszywie pozytywnych wyników ($T_f$). Krzywa idealnego klasyfikatora obejmuje lewy górny punkt wykresu (100% prawdziwie pozytywnych wyników i 0% fałszywie pozytywnych), zatem zazwyczaj większe pole pod wykresem jest lepsze. Istotne jest także nachylenie wykresu, bardziej stromy jest lepszy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "indices = y < 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[indices], y[indices], test_size=0.5, random_state=2)\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "area_under_curve = roc_auc_score(y_test, probs)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, probs)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='no skill')\n",
    "plt.plot(false_positive_rate, true_positive_rate, marker='.', label='ROC curve (area = {0:.2f})'.format(area_under_curve))\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('T_f (false positive rate)')\n",
    "plt.ylabel('T_p (true positive rate)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision-recall\n",
    "Miara precision-recall służy do określania jakości klasyfikatora, szczególnie jeżeli liczebność obu klas znacznie się różni. Składa się z dwóch parametrów:\n",
    "* **precision** ($P$): stosunek liczby prawdziwie pozytywnych wskazań ($T_p$) do liczby prawdziwie pozytywnych i fałszywie pozytywnych wskazań klasyfikatora ($T_p + T_f$)\n",
    "$$ P = \\frac{T_p}{T_p+T_f}$$\n",
    "* **recall** ($R$): stosunek liczby prawdziwie pozytywnych wskazań ($T_p$) do liczby prawdziwie pozytywnych i fałszywie negatywnych wskazań klasyfikatora ($T_p + F_n$)\n",
    "$$ R = \\frac{T_p}{T_p+F_n}$$\n",
    "Obie wartości zależą od progu klasyfikatora.\n",
    "\n",
    "Zmniejszanie progu może zwiększyć ilość zwróconych wyników ($T_p+T_f$). Jeżeli próg był ustawiony zbyt wysoko, to dodatkowe wyniki będą prawdziwie pozytywne, dzięki czemu zwiększy się precision i recall. Jeżeli próg był odpowiedni lub zbyt niski, to w dodatkowych wynikach będzie dużo fałszywie pozytywnych punktów, przez co spadnie precision, a recall niewiele się zwiększy. Obrazuje to kształt wykresu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "# split into train/test sets, selecting only 2 classes\n",
    "indices = y < 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[indices], y[indices], test_size=0.5, random_state=2)\n",
    "\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "# predict probabilities\n",
    "probs = classifier.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# predict class values\n",
    "yhat = classifier.predict(X_test)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "f1 = f1_score(y_test, yhat)\n",
    "area_under_curve = auc(recall, precision)\n",
    "average_precision = average_precision_score(y_test, probs)\n",
    "\n",
    "print('f1=%.3f auc=%.3f ap=%.3f' % (f1, area_under_curve, average_precision))\n",
    "\n",
    "plt.plot([0, 1], [0.5, 0.5], linestyle='--', label='no skill')\n",
    "plt.plot(recall, precision, marker='.', label='precision-recall curve (area = {0:.2f})'.format(area_under_curve))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('R (recall)')\n",
    "plt.ylabel('P (precision)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1\n",
    "F1 to miara jakości klasyfikatora. Jest średnią harmoniczną wartości precision i recall. Najlepszy klasyfikator ma F1 równy 1, a najgorszy 0.\n",
    "$$F1 = (\\frac{P^{-1} + R^{-1}}{2})^{-1} = 2\\frac{P\\cdot R}{P+R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Różnica między krzywymi ROC i precision-recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9,0.1], random_state=1)\n",
    "# split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "# predict probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "roc_auc = roc_auc_score(y_test, probs)\n",
    "print('area under ROC curve: %.3f' % roc_auc)\n",
    "# calculate roc curve\n",
    "false_positive_ratio, true_positive_ratio, thresholds = roc_curve(y_test, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='no skill')\n",
    "plt.plot(false_positive_ratio, true_positive_ratio, marker='.', label='ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('T_f (false positive rate)')\n",
    "plt.ylabel('T_p (true positive rate)')\n",
    "plt.show()\n",
    "\n",
    "# predict class values\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "f1 = f1_score(y_test, yhat)\n",
    "pr_auc = auc(recall, precision)\n",
    "ap = average_precision_score(y_test, probs)\n",
    "print('f1=%.3f auc=%.3f ap=%.3f' % (f1, pr_auc, ap))\n",
    "\n",
    "plt.plot([0, 1], [0.1, 0.1], linestyle='--', label='no skill')\n",
    "plt.plot(recall, precision, marker='.', label='precision-recall curve (area = {0:.2f})'.format(area_under_curve))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('R (recall)')\n",
    "plt.ylabel('P (precision)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MYPwMIOswVh"
   },
   "source": [
    "## Działanie SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqlIAdCatOS8"
   },
   "source": [
    "* Dane ze zbioru WBC dzielimy na K podzbiorów\n",
    "* Dla każdego podzbioru trenujemy SVM, pozostałe podzbiory służą do weryfikacji\n",
    "* Obliczamy parametry działania dla każdego z klasyfikatorów z osobna i łączne, dla czałego zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_all_folds(X, y, k_fold, classifier, title):\n",
    "    print(title)\n",
    "    f, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    y_real = []\n",
    "    y_proba = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(k_fold.split(X, y)):\n",
    "        # split dataset\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # train classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # predict probabilities\n",
    "        proba = classifier.predict_proba(X_test)\n",
    "        proba = proba[:,1]\n",
    "        \n",
    "        # calculate and plot ROC curve\n",
    "        roc_auc = roc_auc_score(y_test, proba)\n",
    "        false_positive_ratio, true_positive_ratio, thresholds = roc_curve(y_test, proba)\n",
    "        roc_label = 'Fold {}: ROC AUC={:.4f}'.format(i+1, roc_auc)\n",
    "        axes[0].step(false_positive_ratio, true_positive_ratio, marker='.', label=roc_label)\n",
    "        \n",
    "        # calculate and plot precision-recall curve, F1 score and average precision\n",
    "        yhat = classifier.predict(X_test)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, proba)\n",
    "        f1 = f1_score(y_test, yhat)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        ap = average_precision_score(y_test, proba)\n",
    "\n",
    "        pr_label = 'Fold {}: PR AUC={:.4f}'.format(i+1, auc(recall, precision))\n",
    "        axes[1].step(recall, precision, marker='.', label=pr_label)\n",
    "    \n",
    "        print('Fold {}: ROC auc={:.4f} f1={:.4f} P-R auc={:.4f} ap={:.4f}'.format(i, roc_auc, f1, pr_auc, ap))\n",
    "        \n",
    "        y_real.append(y_test)\n",
    "        y_proba.append(proba)\n",
    "\n",
    "    # contatenate values from whole dataset\n",
    "    y_real = np.concatenate(y_real)\n",
    "    y_proba = np.concatenate(y_proba)\n",
    "    \n",
    "    # calculate average ROC curve and plot reference value\n",
    "    false_positive_ratio, true_positive_ratio, _ = roc_curve(y_real, y_proba)\n",
    "    roc_label = 'Overall AUC={:.4f} {:.4f}'.format(auc(true_positive_ratio, false_positive_ratio),roc_auc_score(y_real, y_proba) )\n",
    "    axes[0].step(false_positive_ratio, true_positive_ratio, label=roc_label, lw=2, color='black')\n",
    "    axes[0].plot([0, 1], [0, 1], linestyle='--', label='no skill')\n",
    "    \n",
    "    # calculate average precision-recall curve and plot reference value\n",
    "    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n",
    "    pr_label = 'Overall AUC=%.4f' % (auc(recall, precision))\n",
    "    axes[1].step(recall, precision, label=pr_label, lw=2, color='black')\n",
    "    axes[1].plot([0, 1], [0.5, 0.5], linestyle='--', label='no skill')\n",
    "    \n",
    "    axes[0].legend(loc='best', fontsize='small')\n",
    "    axes[0].set_xlabel('T_f (false positive rate)')\n",
    "    axes[0].set_ylabel('T_p (true positive rate)')\n",
    "    axes[1].set_xlabel('R (recall)')\n",
    "    axes[1].set_ylabel('P (precision)')\n",
    "    axes[1].legend(loc='best', fontsize='small')\n",
    "\n",
    "    f.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "FOLDS = 5\n",
    "SAMPLES = 200\n",
    "\n",
    "X = wbc_features[:SAMPLES]\n",
    "y = wbc_targets[:SAMPLES]\n",
    "\n",
    "# prepare classifiers\n",
    "classifiers = [\n",
    "    sk_svm.SVC(gamma='auto', kernel='linear', C=1.0, probability=True, random_state=1),\n",
    "    sk_svm.SVC(gamma=0.001, kernel='rbf', probability=True, random_state=1),\n",
    "    sk_svm.SVC(gamma='auto', kernel='rbf', probability=True, random_state=1),\n",
    "    sk_svm.SVC(gamma='auto', kernel='poly', C=1.0, probability=True, random_state=1),\n",
    "]\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    title = 'SVC kernel: {}'.format(classifier.kernel)\n",
    "    plot_all_folds(X, y, k_fold, classifier, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja wyników za pomocą PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "X = wbc_features[:SAMPLES]\n",
    "y = wbc_targets[:SAMPLES]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "# aplikacja PCA na zbiorze danych X\n",
    "pca.fit(X)\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "for classifier in classifiers:\n",
    "\n",
    "    classifier.fit(X, y)\n",
    "    # plot predictions\n",
    "    plt.scatter(X_transformed[y_pred==0, 0], X_transformed[y_pred==0, 1], color='none', edgecolor='darkred', s=50)\n",
    "    plt.scatter(X_transformed[y_pred==1, 0], X_transformed[y_pred==1, 1], color='none', edgecolor='darkblue', s=50)\n",
    "    \n",
    "    # plot expected values\n",
    "    plt.scatter(X_transformed[y==0, 0], X_transformed[y==0, 1], color='red', s=20)\n",
    "    plt.scatter(X_transformed[y==1, 0], X_transformed[y==1, 1], color='blue', s=20)\n",
    "    plt.title(classifier.kernel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTqS1UB4iMHk"
   },
   "source": [
    "## Zadanie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q2FtFM-Al_RQ",
    "outputId": "28ed6306-3566-4559-863d-81575fa8d32a"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def get_values(final_type=np.int):\n",
    "  \"\"\"\n",
    "  Get arrhythmia database.\n",
    "  \"\"\"\n",
    "  \n",
    "  # 13th column contains too much missing data to be usefull\n",
    "  columns = list(range(13))+list(range(14, 280))\n",
    "\n",
    "  arrhythmia = pandas.read_csv(\n",
    "      'http://archive.ics.uci.edu/ml/machine-learning-databases/' \\\n",
    "      'arrhythmia/arrhythmia.data', header=None, usecols=columns)\n",
    "  arrhythmia = arrhythmia.values\n",
    "\n",
    "  # For other missing values we generate values with Gaussian distribution \n",
    "  # with mean and standard deviation of existing values\n",
    "  def calculate_mean(column):\n",
    "    return column[column != '?'].astype(np.float32).mean()\n",
    "\n",
    "  def calculate_std(column):\n",
    "    return column[column != '?'].astype(np.float32).std()\n",
    "\n",
    "  missing_columns = set()\n",
    "  for i, row in enumerate(arrhythmia):\n",
    "    for j, value in enumerate(row):\n",
    "      if value == '?':\n",
    "        missing_columns.add(j)\n",
    "\n",
    "  for col in missing_columns:\n",
    "    mean = calculate_mean(arrhythmia[:, col])\n",
    "    std = calculate_std(arrhythmia[:, col])\n",
    "    for j in range(arrhythmia.shape[0]):\n",
    "      if arrhythmia[j, col] == '?':\n",
    "        arrhythmia[j, col] = np.random.normal(mean, std)\n",
    "\n",
    "  features = arrhythmia[:,:278].astype(final_type)\n",
    "  # We substitute one to get nicer {0,1} classes for binary classification\n",
    "  targets = arrhythmia[:,278].astype(np.int) - 1\n",
    "  \n",
    "  # Remove classes with too few data\n",
    "  for i in range(15):\n",
    "    if np.sum(targets == i) < 5:\n",
    "      features = features[targets != i,:]\n",
    "      targets = targets[targets != i]\n",
    "  return features, targets\n",
    "\n",
    "features, targets = get_values()\n",
    "binary_targets = np.copy(targets)\n",
    "binary_targets[binary_targets > 0] = 1\n",
    "\n",
    "print('Arrhythmia: Zbiór {} danych z {} cechami'.format(*features.shape))\n",
    "\n",
    "# Number of cross-validation splits\n",
    "splits = 5\n",
    "\n",
    "# Metrics to report\n",
    "# ==============================================================================\n",
    "# TODO Add aditional metrics\n",
    "\n",
    "def accuracy(result, target):\n",
    "  return np.sum(result == target) / len(target) * 100\n",
    "\n",
    "metrics = [accuracy]\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def test_classifier(builder, features, labels, metrics=metrics, *args, **kwargs):\n",
    "  \"\"\"\n",
    "  builder - function producing the classifier to test\n",
    "  args and kwargs will be passed to the builder\n",
    "  \n",
    "  Returns mean tuple (mean_accuracy, accuracy_deviation)\n",
    "  \"\"\"\n",
    "  \n",
    "  # Stratified KFold mainains class distribution in iterations\n",
    "  fold = sk.model_selection.StratifiedKFold(splits)\n",
    "  \n",
    "  scores = []\n",
    "  for train, test in fold.split(features, labels):\n",
    "    # Build the model\n",
    "    classifier = builder(*args, **kwargs)\n",
    "    # Train it\n",
    "    classifier.fit(features[train], labels[train])\n",
    "    # Use it to predict test lables\n",
    "    predicted = classifier.predict(features[test])\n",
    "    score = [m(predicted, labels[test]) for m in metrics]\n",
    "    scores.append(score)\n",
    "  \n",
    "  # Report the results as mean +/- standard deviation\n",
    "  scores = np.array(scores)\n",
    "  means = scores.mean(axis=0)\n",
    "  stds = scores.std(axis=0)\n",
    "  for i, metric in enumerate(metrics):\n",
    "    print('{}: {:2.2f}%(+/-{:2.2f}%)'.format(metric.__name__,\n",
    "                                             means[i], stds[i]))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bHrP4d5fEXGv",
    "outputId": "e39ca012-ca30-469c-fb03-9ac9fdc4d739"
   },
   "outputs": [],
   "source": [
    "# Test linear classifier\n",
    "def linear_svm_builder(): \n",
    "  return sk_svm.SVC(kernel='linear')\n",
    "\n",
    "print('Simple linear classifier:')\n",
    "test_classifier(linear_svm_builder, features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6h6kz8XRH_vo"
   },
   "outputs": [],
   "source": [
    "# TODO Test SVM classifiers for different kernels (poly, sigmoid, etc)\n",
    "#      and different kernel parameters (degree, gamma, C)\n",
    "# TODO Compare with k-NN for k=1,3,5\n",
    "# TODO Apply PCA"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pum-lab3-agh.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
